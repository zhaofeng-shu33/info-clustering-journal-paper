%%%%%%%%%%%%%%%%%%%%%%% file template.tex %%%%%%%%%%%%%%%%%%%%%%%%%
%\left( 
% This is a general template file for the LaTeX package SVJour3
% for Springer journals.          Springer Heidelberg 2010/09/16
%
% Copy it to a new file with a new name and use it as the basis
% for your article. Delete % signs as needed.
%
% This template includes a few options for different layouts and
% content for various journals. Please consult a previous issue of
% your journal as needed.
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% First comes an example EPS file -- just ignore it and
% proceed on the \documentclass line
% your LaTeX will extract the file if required
\begin{filecontents*}{example.eps}
%!PS-Adobe-3.0 EPSF-3.0
%%BoundingBox: 19 19 221 221
%%CreationDate: Mon Sep 29 1997
%%Creator: programmed by hand (JK)
%%EndComments
gsave
newpath
  20 20 moveto
  20 220 lineto
  220 220 lineto
  220 20 lineto
closepath
2 setlinewidth
gsave
  .4 setgray fill
grestore
stroke
grestore
\end{filecontents*}
%
\RequirePackage{fix-cm}
%
%\documentclass{svjour3}                     % onecolumn (standard format)
%\documentclass[smallcondensed]{svjour3}     % onecolumn (ditto)
\documentclass[smallextended]{svjour3}       % onecolumn (second format)
%\documentclass[twocolumn]{svjour3}          % twocolumn
%
\smartqed  % flush right qed marks, e.g. at end of proof
%
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{natbib}
\usepackage{bm}
\usepackage{subcaption}
\captionsetup{compatibility=false}
\input{macros.tex}
%
% \usepackage{mathptmx}      % use Times fonts if available on your TeX system
%
% insert here the call for the packages your document requires
%\usepackage{latexsym}
% etc.
%
% please place your own definitions here and don't use \def but
% \newcommand{}{}
%
% Insert the name of "your journal" with
\journalname{Machine Learning}
%
\begin{document}

\title{Graph-Based Info-Clustering and its Applications%\thanks{Grants or other notes
%about the article that should go on the front page should be
%placed here. General acknowledgments should be placed at the end of the article.}
}
%\subtitle{Do you have a subtitle?\\ If so, write it here}

%\titlerunning{Short form of title}        % if too long for running head

\author{Feng Zhao         \and
        Yang Li \and %etc.
        Fei Ma \and
        Shao-Lun Huang \and
        Lin Zhang
}

%\authorrunning{Short form of author list} % if too long for running head

\institute{Feng Zhao \at
              Department of Electronic Engineering, Tsinghua University, Beijing, PR China \\
              \email{zhaof17@mails.tsinghua.edu.cn}           %  \\
%             \emph{Present address:} of F. Author  %  if needed
           \and
           S. Author \at
              second address
}

\date{Received: date / Accepted: date}
% The correct dates will be entered by the editor


\maketitle

\begin{abstract}
We propose a graph-based hierarchical clustering method based on a multivariate information metric.
The proposed method can generate non-binary hierarchical tree that reveals the intrinsic structures in the data, with no hyper-parameter to tune. 
The hierarchical tree can be computed efficiently using
our improved algorithm. Besides clustering task, we show that our method can be adopted and
used in many other unsuperivised data mining tasks including community discovery problem, link prediction and outlier detection.
Experiments show that the clustering result outperforms
other hierarchical clustering techniques and is very suitable to find the complex community structure.
\keywords{Clustering \and Multivariate Mutual Information \and Principal Sequence of Partition}
% \PACS{PACS code1 \and PACS code2 \and more}
% \subclass{MSC code1 \and MSC code2 \and more}
\end{abstract}

\section{Introduction}
\label{intro}
% background information goes here
Clustering Analysis is an important research topic in machine learning.
It has wide applications in many areas. For example, to track the epidemic
sources within a community and assist for the model of natural disasters with observed data.

There are many different methods for clustering analysis. One of them is called
hierarchical clustering. The classical method uses the Euclid distance between samples
as the metric to merge the most similar pairs until all the samples are merged into a clustering
tree \citep{slink}. Compared with other clustering method, hierarchical clustering can better describe the relationship between different clusters. Still, the classical hierarchical clustering method has some
drawbacks. One of them is pairwise merging, which is not optimal since it does not consider information shared among more than two clusters. Besides, the criterion to split the clustering tree into a flat clustering structure is arbitrary set and lacks explanations, which bring some mysteries in practical
usage.

In recent 20 years, there are many emerging hierarchical clustering methods which brought new insights into this area. These new methods arise from combination of different domain knowledge and
subject branch. To name but a few, hierarchical density estimation \citep{hde}, Bayesian hierarchical clustering \citep{bhc},
clusterpath based on convex optimization \citep{hocking2011clusterpath},
graph partition based on Dasgupta's cost \citep{dasgupta2016cost},
Gradient-based Hierarchical Clustering based on hyperbolic gemetry \citep{hyperbolic} etc.
These methods have both advantages and disadvantages.
In practice researchers need to choose the proper method according to the specific problem.
For example, an extension called Bayesian Rose Trees of Bayesian clustering
can produce non-binary hierarchical tree
under a given probabilistic model \citep{blundell2011discovering}.
But Bayesian model has many hyper-parameters to tune and the clustering results between different runs vary greatly because of its intrinsic randomness.


Besides the hierarchical methods mentioned above, there is a specific branch which brings information theory into clustering. This branch is based on the observation that metrics in hierarchical clusterings
are mostly empirically inspired and may not reflect the true underlining probalistic model of the data.
Either variants of mutual information or some other kind of information-theoretic metric \citep{ic2002}  is used in the past literature. For the former method, the metric of mutual information is used and an agglomerative scheme is adopted \citep{mim}. The mutual information between samples and cluster labels is estimated using Parzen' window, which is a kind of Gaussian mixture model. Such adoption
has some theoretical benefits but suffers from sub-optimization of mutual information cost in the agglomerative approximation. It is until the proposal of multivariate mutual information metric in clustering that there is some breakthrough \citep{ic2016}.

Multivariate mutual information, as its name suggests, is an extension of mutual information for multiple
random variables. Using such metric as the thresholds, \citet{ic2016} shows that we can get a clustering
tree for random variables. The theory sounds good but it is nearly impossible to compute such metric
in practice. Similar issues exist for Bayesian probabilistic approach. For these probablistic methods, we
are unable to estimate precisely the distribution from data. While prametric model is used in Bayesian
method, we will propose a graph model to apply multivariate mutual information for clustering.

Reducing from abstract probablistic space to graph model is not a crazy idea but origins from the local regime assumption of distributions. It's known that estimation of mutual information between $X$ and $Y$ from data is a hard task but when $X$ and $Y$ are nearly indepedent, \citet{huang2017information} shows that the estimation can be simplied to do some kind of alternative conditional expectation, which is much easier
than estimating probabilty distribution function. In this paper, we will extend this local regime result to multivariate case and show how it is equivalent to a graph model.

Many problems in graph model are NP-Hard and only allows approximation solutions. Quite surprisingly, our proposed graph-based clustering method is polynomial-time efficient solvable exactly.
This result has been established by using a special graph partition structure called "Principal Sequence of Partition"(PSP), which is equivalent to our divisive approach. The compution of "Principal Lattice" allows a polynomial time algorithm.
However, the original algorithm proposed by
\citet{narayanan}, which is used directly by \citet{mac}, has a high time complexity to fit the practice. There has been some improvement
using parametric maximal flow techniques \citep{kolmogorov}, which is later adopted for graph model
by \citet{pin}. Though these two improvements share the same time complexity, we will show that our improvement runs almost an order of magnitude faster than the parametric approach under a fair implementation environment.

There has been little prior work on applying Info-Clustering methods to data engineering problems except for some proof-of-concept experiment \citep{mac}. In this paper, we will consider the application
of our graph-based Info-Clustering to different unsupervised learning problems. Specifically
we will not only consider data clustering problems, in which our proposed method can be directly applied, but also for outlier detection and community discovery problems we will propose new promising
method adopted from graph-based Info-Clustering. We will conduct empirical experiments on both artificially generated data and some real-world dataset and show the feasibility of our method in
data engineering problems.

The paper is organized as follows: In Section \ref{sec:bk} we introduce the backgrond knowledge of the theorical Info-Clustering, its relationship with PSP structure and theory of local geometry information. In Section \ref{sec:GBIC} our formulation of graph-based Info-Clustering(GBIC) and important properties of our model are given. In Section \ref{sec:alg}, we propose an improved algorithm to compute the PSP structure of graph. Then in Section \ref{sec:es} we conduction empirical study of GBIC on data clustering, outlier detection and community discovery. Finally we give the conclusion in Section \ref{sec:conc}.


\section{Background Knowledge}\label{sec:bk}
Before proceeding to introduce our theoretical and algorithmic results, it is necessary to review some prior results from minimal technical aspect. We split this section into three parts, first we introduce
the local information geometry theory and the concept of weak dependency of two random variables;
next is the general info-clustering model and its relationship with principal sequence of partitions (PSP) of submodular function;
the final part is about the original algorithm used to solve PSP problem when the submodular function
is graph cut function.
\subsection{Local Information Geometry}
It is well known that KL-Divengence can be used to measure the distance of two probabistic distributions. When the two distributions are quite "near", that is, both of them are within a neighborhood of a distribution $P$, we can simplify the expression of KL-Divergence as the Euclid distance \citep{huang2017information}. To be more specific, we use the example of discrete distribution and use $P_0$ to represent a distribution on alphabet $\mathcal{X}$. We say $P$ is in a $\epsilon$-Neighborhood $N^{(\epsilon)}(P_0)$ of $P_0$ if
\begin{equation}
P \in N^{(\epsilon)}(P_0) \iff \sum_{x \in \mathcal{X}} \frac{(P(x) - P_0(x))^2}{P(x)} < \epsilon^2
\end{equation}
The above equation can be expressed in concise form if we write $P(x) = P_0(x) + \epsilon
\sqrt{P_0(x)} \phi(x)$, in which $\phi(x)$ is treated as feature vector. Then we have $||\phi || < 1$.
If both $P_1, P_2 \in N^{(\epsilon)}(P_0)$ we have
\begin{equation}\label{eq:approx:ig}
D(P_1 || P_2) = \frac{\epsilon^2}{2} ||\phi_1 - \phi_2||^2 + o(\epsilon^2)
\end{equation}
In addition, the above result can be used when two random variables are weakly independent \citep{huang2019universal}. That is, we say $X$ and $Y$ are weakly dependent if the conditional distribution $Y|X(\cdot |x) \in N^{(\epsilon)}(Y)$ for all $x \in \mathcal{X}$. Under the assumption of
weak dependence the mutual information, which is defined by $D(P_{XY}||P_XP_Y)$ can be expressed
in concise form:
\begin{equation}\label{eq:Ixy}
I(X;Y) = \frac{1}{2}\sum_{x\in \mathcal{X}, y\in \mathcal{Y}} B^2(x,y) + o(\epsilon^2) \textrm{ where }  B(x,y)=\frac{P_{X,Y}(x,y) - P_X(x) P_Y(y)}{\sqrt{P_X(x)P_Y(y)}}
\end{equation}
To simplify the notation, the term $\sum_{x\in \mathcal{X}, y\in \mathcal{Y}} B^2(x,y)$ can be regarded as the square of Frobenious norm $\norm{B}_F^2$ for matrix $\bm{B}$. Equation \eqref{eq:Ixy} gives the domaint term
of the mutual information when the random variables are weakly dependent.

\subsection{Theoretical Info-Clustering}
There are many ways to extend mutual information for multiple random variables. \cite{ic2016} gives one based on partition of set $V$. All partitions of $V$ except for $\{V\}$ consists the non-trivial partition collection $\Pi'(V)$ and the multivariate mutual information(MMI) of $\{Z_i | i \in V\}$ is defined as:
\begin{align}\label{eq:IZV}
I(Z_V) &:= \min_{P \in \Pi'(V)} I_{P}(Z_V) \textrm{ where} \\
I_P(Z_V) &:= \frac{1}{|P| - 1}D(P_{Z_V} || \prod_{C\in P} P_{Z_C})
\end{align}
Based on the definition of MMI, the cluster can be obtained by considering all maximal subsets $B$ such that $I(Z_B) > \lambda$ for a certain threshold $\lambda$. All such $B$ consists of $\mathcal{C}(Z_V)$.
This is the theoretical definition of clusters, which is equivalent to the algorithmic one:

\begin{equation}\label{eq:CZV}
C(Z_V) = \{ V\} \cup \bigcup_{B \in \mathcal{C}_{\lambda_1}} \mathcal{C}(Z_B) \textrm{ where } \lambda_1 = I(Z_V)
\end{equation}

Equation \eqref{eq:CZV} gives a divisive approach to obtain all clusters and threshold values from
the largest set $V$ and smallest threshold value $\lambda_1$. However, we cannot get a feasible algorithm
to compute the clusters since it is unknown how to compute $I(Z_V)$.
To get over this difficulty a relationship between $I(Z_V)$ and a special kind of submodular function minimization problem is established \citep{mac}: All threshold values and clusters are the same as those of
the following optimization problem with $f[\mathcal{P}]=D(P_{Z_V} || \prod_{C\in P}P_{Z_C})$:
\begin{align}\label{eq:hPLambda}
h_{\mathcal{P}}(\lambda) &= f[\mathcal{P}] - |\mathcal{P}| \lambda \notag\\
h(\lambda) &= \min_{\mathcal{P} \in \Pi'(V)}h_{\mathcal{P}}(\lambda)
\end{align}
In the above equation, $h(\lambda)$ is a piecewise linear function about $\lambda$ with turning points
$\lambda_1, \dots, \lambda_{k}$. The general solution for \eqref{eq:hPLambda} can be written as
\begin{equation}\label{eq:PSP_structure}
h(\lambda) = \begin{cases} h_{P_0}(\lambda) & \lambda < \lambda_1 \\
h_{P_i}(\lambda) & \lambda_i \leq \lambda < \lambda_{i+1} \textrm{ for } i = 1, \dots, k-1 \\
h_{P_k}(\lambda) & \lambda \geq \lambda_k
\end{cases}
\end{equation}
Furthermore, we have $P_0 \succeq P_1 \dots \succeq P_k$ \citep{narayanan}. That is, partition $P_{i+1}$ is a refinement of $P_i$.

Another link between MMI and threshold values is established by \citet{agg_ic} as 
\begin{equation}\label{eq:largest_threshold}
\lambda_k = \max_{B\subseteq V} I(Z_B)
\end{equation}
\subsection{Efficient Algorithm to solve PSP structure}
\cite{narayanan} proposes a divide and conquer approach to solve the PSP structure from equation \eqref{eq:hPLambda} for a general submodular function. Since the optimal partition is $P_0 = \{V\}$ when $\lambda=0$ and the partition is $P_k=\{\{1\}, \{2\}, \dots, \{|V|\}\}$ when $\lambda$ is quite large,
his method starts from two endpoints $\lambda_i = 0, \lambda_j = +\infty$ and find the intersection of 
$h(\lambda_i)$ and $h(\lambda_j)$ at $(\lambda', h')$. Then we compute a partition $P'$ such that $h(\lambda') = h_{P'}(\lambda')$. By definition $h(\lambda') \leq h'$ and when the strict inequality is taken we can repeat the process within $(\lambda_i, \lambda')$ and $(\lambda', \lambda_j)$ respectively. The technique procedure is documented in Algorithm \ref{alg:psp}.
\begin{algorithm}
\caption{PSP algorithm}\label{alg:psp}
\begin{algorithmic}[1]
\REQUIRE Statistics of $Z_V$ sufficient for calculating $f(B)$ for all $B \subseteq V$
\ENSURE A sorted critical value array \textbf{L} and a reverse ordered array \textbf{PSP} containing $\P_1,\dots, P_k$.
\STATE \textbf{L}  $\leftarrow$ empty list.
\STATE $Q\leftarrow \{V\}, P \leftarrow \{ \{i \} | i \in V\}$
\STATE $\mathbf{PSP}= [Q, P]$
\STATE \texttt{Split}$(Q,P)$
\STATE sort $L$ and sort $\mathbf{PSP}$ with respect to $\succeq$ 
\FUNCTION{\texttt{Split}$(Q,P)$}
 \STATE\label{alg:gamma} $\gamma' = {1 \over \abs{P} - \abs{Q}} (f(P)-f(Q))$
 \STATE $h' = {1 \over \abs{P} - \abs{Q}}(\abs{P} f(Q) - \abs{Q} f(P))$
 \STATE $(\tilde{h}, P') = \texttt{DT}(f,\gamma')$ \footnotemark
 \IF{$\tilde{h} = h'$}
 	\STATE insert $\gamma'$ to $\mathbf{L}$
 \ELSE
 	\STATE insert $P'$ to $\mathbf{PSP}$
 	\STATE \texttt{Split}$(Q, P')$
 	\STATE \texttt{Split}$(P',P)$
 \ENDIF
\ENDFUNCTION
\end{algorithmic}
\end{algorithm}

In Algorithm \ref{alg:psp}, a function DT is used to get the miminal value. There are some candicates for this function, throughout this paper we use the one proposed by \cite{mac}. Since the theoretical and algorithmic detail is not correlated with this paper, we omit their introduction.

\section{Formulation of graph-based Info-clustering}\label{sec:GBIC}

General model of Info-Clustering is not very useful since we know little about the joint distribution.
We need to do some kind of model reduction to get some useful model. In particular, it has been shown
by \cite{ic2016} that when $Z_1, \dots, Z_n$ forms a Markov chain, then we can get a model called Mutual Information Relevance Network \citep{butte1999mutual}. In the following section we will show
that the general model reduces to our graph-based Info-Clustering when $Z_1, \dots, Z_n$ are weakly
independent. The relationship of these two special models, together with their parent model, are shown
in the Vienn diagram in Fig. \ref{fig:relationship} % For one-column wide figures use

\begin{figure}
\centering
% Use the relevant command to insert your figure file.
% For example, with the graphicx package use
  \includegraphics[width=6cm]{relationship.eps}
% figure caption is below the figuregit
\caption{General Info clustering models reduces to GBIC or MIRN model respectively under different assumptions for the joint distribution}
\label{fig:relationship}       % Give a unique label
\end{figure}

From local geometry information it is known that two random variables $X$ and $Y$ are weakly dependent if
\begin{equation}
P_{Y|X=x}(y) = P_Y(y) +\epsilon \phi_x(y) \sqrt{P_Y(y)}
\end{equation}
where $\norm{\phi_x}\leq 1$.

For multiple random variables we give:

\begin{definition}\label{def:general}
$X_1, \dots, X_n (n\geq 3)$ are weakly dependent if there exist a random variable $U$ such that
$(X_1, \dots, X_n)$ is weakly dependent with $U$ and $X_1, \dots X_n$ are independent conditioned
on $U$.
\end{definition}

Let $\P$ be the partition of $V=\{1,2,\dots, n\}$. We claim:
\begin{theorem}\label{thm:DPX}
If $X_1, \dots, X_n$ are weakly dependent, then
\begin{equation}
D(P_{X_V} || \prod_{C\in \P} P_{X_{C}}) = {1 \over 2}\sum_{\substack{(i,j) \not\in C\\ C\in \P}} \norm{B_{ij}}_F^2 + o(\epsilon^2)
\end{equation}
\end{theorem}
We can see that Theorem \ref{thm:DPX} is an extension of the approximation of mutual equation in Equation \eqref{eq:Ixy} to the case of multiple random variables. We can treat the B matrix term ${1 \over 2} \norm{B_{ij}}_F^2$
as the capacity weight for a graph. To get the required KL-Divergence under a given partition, we only need to make a
summation of these weight terms.

Theorem \ref{thm:DPX} inspires us to get a simpler model
called Graph-Based Info-Clustering from
Equation \eqref{eq:IZV}.

Consider a directed graph
\footnotetext{without loss of generality we only consider directly graph to reduce computing complexity in algorithm part}
$G(V, E)$ where the edge weight $w$ is nonnegative and represents pairwise similarity of nodes. For a subset $C$ of $V$, $f(\cdot)$ is graph cut function, that is $f(C) = \sum_{i\not\in C, j\in C, (i,j) \in E} w_{ij}$. To measure the similarity shared by multiple nodes, we use the following metric.
\begin{definition}[multivariate similarity]\label{def:ms}
\begin{align}
I_{\P}(Z_V) & := \frac{ f[\P] }{  \abs{\mathcal{P}} - 1 } \textrm{ where } f[\P] = \sum_{C\in \P}f(C)\\
I(Z_V) & := \min_{\mathcal{P} \in \Pi'(V)} I_{\mathcal{P}}(Z_V)  \label{eq:ms}
\end{align}
\end{definition}
The quantity $I(Z_V)$ in equation \eqref{eq:ms} is called \textbf{multivariate similarity} of $G$. We can also compute multivariate similarity for subgraph $G'(V',E')$ of $G$, denoted by $I(Z_{V'})$. Multivariate similarity is an extension of pairwise similarity. When $V'=\{i,j\}$, we have $I(Z_{V'})=w_{ij}$ from above definition.

To perform clustering tasks using our multivariate similarity we proceed in a divisive way as Equation \eqref{eq:CZV}. Instead of focusing on each clusters, we are trying to get the cluster tree $\mathcal{T}$. A hierarchical tree $\mathcal{T}$ of $G$ is a rooted tree. Each node in $\mathcal{T}$ is represented by a subset $S \subset V$. Its root node is $V$ and its leaf node is $\{j\}$. Each non-leaf node of $\mathcal{T}$ is called a cluster of graph $G$.

For a graph $G$, suppose $I_{\P^*}(Z_V)=I(Z_V)$, each element of $\P^*$ is a child of the hierarchical tree root $V$. For each child node set $C$, use partition $\P_C$ where $I(Z_C)=I_{\P_C}(Z_C)$ to split it until this tree branch is divided into singletons.

Besides the above top-down approach, there is an equivalent "bottom-up" build procedure inspired from Equation \eqref{eq:largest_threshold}: For a graph $G$, suppose $I(Z_C) = \max_{B\subseteq V} I(Z_B)$ and $C$ is maximal, merge elements of $C$ together and contract $G$ to a smaller graph. At each step select the set with maximal multivariate similarity to contract the graph until the graph is contracted to a single node, which is the tree root.

The two procedures are illustrated by Fig. \ref{fig:ta}. For the top-down approach, there may be multiple partition $\P$ that reaches the minimum in \eqref{eq:ms}, we can show that there is a unique finest partition and this one is used; for the bottom-up approach, there may be multiple intersecting subsets with equal maximal multivariate similarity, we can show that the union of these subsets also reach the maximal and maximal $C$ is well defined.
%proof in attachment, before the proof of the above proposition. (2 statements)
\begin{figure}
\centering
\includegraphics[width=\textwidth]{two_approach.eps}
\caption{hierarchical tree generation based on multivariate similarity. Left figure shows the top-down approach with each subset split into multiple sets. Right figure shows the bottom-up approach with the graph nodes $v_1,v_2,v_3$ contracted to $C$. The weight between $C$ and node $u$ is $w_1+w_2+w_3$.}\label{fig:ta}
\end{figure}

By the construction of the clustering tree, we can associate each tree node with a threshold value, which is the multivariate similarity computed at that step.
\begin{example}
\begin{figure}
\centering
\includegraphics[width=0.7\textwidth]{threshold.eps}
\caption{a clustering tree (left) for the weighted graph (right) by graph-based info-clustering method. Threshold values are $0.9, 1, 2$.}\label{fig:threshold}
\end{figure}
Consider an undirected graph $G(V, E)$ with $V=\{1,2,3,4\}$ and $E=\{(1,2),(1,3),(3,4),(2,4)\}$ with edge weight $w_{12}=1,w_{13}=0.4,w_{24}=0.5,w_{34}=2$ (See Fig. \ref{fig:threshold}). The threshold values are the multivariate similarity $I(Z_{3,4})=2, I(Z_{1,2})=1$ and $I(Z_V)=0.9$ respectively. We can write the complete clustering solution as:
\begin{equation*}
\P = 
\begin{cases}
\{\{1,2,3,4\}\} & \lambda < 0.9 \\
\{\{1,2\},\{3,4\}\} & 0.9 \leq \lambda < 1 \\
\{\{1\},\{2\},\{3,4\}\} & 1 \leq \lambda < 2\\
\{\{1\},\{2\},\{3\},\{4\}\} & \lambda \geq 2
\end{cases}
\end{equation*}
Although the clustering tree does not distinguish $\{\{1\},\{2\},\{3,4\}\}$ from $\{\{1,2\},\{3\}, \{4\}\}$, the partition $\{\{1,2\},\{3\}, \{4\}\}$ does not exist.
\end{example}

Info-clustering assumes that the weight is additive and tends to produce trivial hierarchical structure when the value of weights are near equal. This property is shown by the following theorem.
\begin{theorem}\label{thm:trival}
The partition to minimize equation \eqref{eq:ms} is $\P_k = \{\{1\},\{2\},\dots,\{\abs{V}\}\}$ if and only if equation \eqref{eq:GF} holds.
\begin{equation}\label{eq:GF}
\frac{f[\P]}{\abs{\P}-1} \geq \frac{f[\P_k]}{\abs{V}-1} \textrm{ for any } \P \in \Pi'
\end{equation}
\end{theorem}
\begin{proof}
We use Algorithm \ref{alg:psp}. Since $h(\lambda)$ is piecewise linear, the first line segment of $h(\lambda)$ is $ - \lambda $ and the last line segment is $ f[\P_k] - \abs{V} \lambda$. Their intersection point is $(\lambda_{+}, -\lambda_{+})$ where $\lambda_{+} = \frac{f[\P_k]}{\abs{V}-1}$. Since $\frac{f[\P]}{\abs{\P}-1} \geq \lambda_{+} \iff f[\P] - \abs{\P}\lambda_{+} \geq - \lambda_{+}$ Equation \eqref{eq:GF} says the minimizer of $h(\lambda)$ at $\lambda = \lambda_{+}$ is either $\{V\}$ or $\{\{1\},\{2\},\dots,\{\abs{V}\}\}$ and $h(\lambda)$ has only two line segments. From geometric point of view, the solution to equation \eqref{eq:ms} is the first turning point of $h(\lambda)$ and it is equal to that of last turning point if and only if \eqref{eq:GF} holds.
\end{proof}
By Theorem \ref{thm:trival}, we can show that if all graph weights are of equal value, then by applying GBIC we can only get trivial cluster $\{V\}$. Generally, we have the following proposition.

\begin{proposition}\label{prop:triangle}
Let $w_{ij}=0$ if $(i,j)\not\in E$. If $w_{ij} + w_{jk} \geq w_{ki}$ for any different triple $i, j, k \in V$, then the info-clustering solution is trivial\footnotemark.
\end{proposition}
\footnotetext{By trivial solution we mean only $\{V\}$ is a cluster, or the hierarchical tree contains only root and leaves.}


Proposition \ref{prop:triangle} inspires us in order to separate different clusters, we need small magnitude of inter-cluster weights and large similar intra-cluster weight. 

Using Proposition \ref{prop:triangle}, a special case is a complete graph $G(V,E)$ with equal weight $r$ on each edge. We then have $I(Z_{V})=\frac{nr}{2}$ where $n=\abs{V}$. If we add some edge between such two complete graphs, an interesting problem is to investigate when GBIC can recover the two subgraphs. A subset is recoverable if this set is a cluster produced in GBIC model.
For this problem, we have the following proposition:

\begin{proposition}\label{prop:reweight}
Suppose $S_1, S_2 $ are complete graph node set with $\abs{S_1}=\abs{S_2}=n$ and equal weight $w_{ij}=r$. There are $m$ edges between the two graphs and all inter-connection edges have equal weight 1. The vertex set for the whole graph is $V=S_1\cup S_2$. If $m <\frac{nr}{2}$  we have
$I(Z_V) = 
m$ and the recovery is possible. If $m\geq \frac{nr}{2}$, the recovery is not possible.
\end{proposition}

In later part we will show how the result of Proposition \ref{prop:reweight} can be used to build a hierarhcial discovery method in community detection problems.

Besides considering static graph $G$, there is often a problem to consider which cluster the new observation belongs to.
To model this, we add a new node $i'$ to the existing graph $G$.
Let $V'=B \cup \{i'\}$, we can compute $\gamma'_N$ for the new graph $G(V', E(V'))$ and compare it with $\gamma_N$. From equation \eqref{eq:largest_threshold} $\gamma'_N \geq \gamma_N$ holds in general. It can be shown that we do not need to compute $\gamma'_N$ to determine whether $\gamma'_N>\gamma_N$. We summarize our main result as follows:
\begin{proposition}\label{prop:main}
\begin{equation}
\gamma'_N > \gamma_N \iff  \sum_{i \in B} w_{ii'} > \gamma_N 
\end{equation}
\end{proposition}
Proposition \ref{prop:main} gave a method to check whether the newly added node destroys the outermost structure of the original clustering tree.  In later part we will show how this result can be used to build our Info-Detection method for outlier detection problem.
\section{Improved PSP Algorithms}\label{sec:alg}
To compute the clusters in GBIC, we use an indrect method. That is, we first compute the PSP structure defined in Equation \eqref{eq:PSP_structure}. Then from the established equivalence of the nested partition and the clustering tree, the hierarchical
clustering results are obtained.

\subsection{Formulation of PSPI}
Compared with Algorithm \ref{alg:psp}, we will give an improved algorithm in this Section using the same \texttt{DT} routine. We notice that different invocation of DT is independent and does not utilize the intrinsic hierarchical structure in the original version of PSP algorithm \cite{narayanan}. If the hierarchical tree structure is used, we can invoke DT on smaller graph in later computation and make the total computation one order of magnitude faster than previous.

To be more specific, suppose we get $P_i = {C_1, \dots, C_t}$ for the first invocation of DT. Then we can compute PSP for each $C_i(i=1,\dots, t)$ respectively and construct those $P_j(j>i)$ from the subgraph computation. For $P_j(j<i)$, we can contract the graph $G$  to $G^t$ which has $t$ nodes by contracting each $C_i$ to a single node. By applying DT to $G^t$ can we get $P_j(j<i)$. We summarize our improved version (PSPI) in Algorithm \ref{alg:psp_i_simplified}.

\begin{algorithm}
	\caption{An Improved Principal Sequence of Partition Algorithm}\label{alg:psp_i_simplified}
	\begin{algorithmic}[1]
		\REQUIRE a directed graph $G(V, E)$; edge weight map $c(e)$ for $e\in E$
		\ENSURE a hierarchical tree $\mathcal{T}(K, E)$ where $K \subseteq 2^{V}$ is node set and $E$ is edge set
		\STATE initialize tree $\mathcal{T}$ with $V$ as root node, $\{j\}(j \in V)$ as leaf node and no stem node
		\STATE \texttt{Split}($G, V$)
		\FUNCTION{\texttt{Split}($\widetilde{G}, \widetilde{V}$)}
		\STATE $w$ is the summation of all edge weights of $\widetilde{G}$ 
		\STATE $\gamma' = \frac{w}{\abs{V(\widetilde{G})}-1}$ where $V(\widetilde{G})$ is the node set of graph $\widetilde{G}$ \label{alg:gamma_apostrophe}
		\STATE $(\tilde{h}, P') = \texttt{DT}(\widetilde{G}, \gamma')$ where $\P'$ is minimizer of $h(\gamma')$ in equation \eqref{eq:hPLambda} and $\tilde{h}$ is the corresponding minimum value  \label{line:DT}
		\IF{$\tilde{h} = - \gamma'$}
		\STATE add edge weight $\gamma'$ in $\mathcal{T}$ starting from $\widetilde{V}$ to its children
		\ELSE
		\FOR{$S$ in $P'$ and $\abs{S}>1$}
		\STATE make each children of $\widetilde{V}$ in $\mathcal{T}$ have new parent $S$		
		\STATE make the parent of $S$ be $\widetilde{V}$
		\STATE \texttt{Split}($\widetilde{G}[S], S$) where $\widetilde{G}[S]$ is the subgraph of $\widetilde{G}$ restricted on $S$
		\STATE contract $S$ to a single node in $\widetilde{G}$ % graph \widetilde{G} is modified
		\ENDFOR 
		\STATE \texttt{Split}($\widetilde{G}, \widetilde{V}$)		
		\ENDIF
		\ENDFUNCTION
	\end{algorithmic}
\end{algorithm}

\begin{figure}
\centering
\includegraphics[width=0.9\textwidth]{improved_alg.png}
\caption{Illustration of PSPI: The intermediate state is achieved first by Line \ref{line:DT} in Algorithm \ref{alg:psp_i_simplified};
Then the larger ''ellipse'' is divided further and the tree ``ellipses'' are clustered upwards in a prescribed order to formal the final state.}\label{fig:pspi}
\end{figure}

We analyze the time complexity of Algorithm \ref{alg:psp_i_simplified} for dense graphs. That is, we suppose $\abs{V} = n, \abs{E} = O(n^2)$. For sparse graphs our algorithm is faster. The highest-relabel preflow algorithm is used to solve the maximum flow problem. Then \texttt{DT} routine has $O(n^4)$ time complexity. The time complexity for Algorithm \ref{alg:psp} is $O(n^5)$.
We use $T(n)$ to represent the time complexity of \texttt{Split} when the graph has $n$ nodes.

The recursive relationship for $T(n)$ is
\begin{equation}\label{eq:Tn}
T(n) = \max \{ C n^4 + \sum_{i=1}^k T(n_i) + T(k)\delta_{k<n} | \sum_{i=1}^k n_i = n, n_i \in \mathbb{Z}_{+} \}
\end{equation}	
In \eqref{eq:Tn}, $Cn^4$ is the time complexity of \texttt{DT} when $\abs{E} = O(n^2)$. $\delta_{k<n} = 1$ when $k<n$ otherwise $\delta_{k<n}=0$. We have the following proposition:
\begin{proposition}\label{prop:alg_complexity}
	 If $n_i \leq \frac{n}{2}, \textrm{ for } i=1,\dots,k$ and $ k \leq \frac{n}{2}$  hold in equation \eqref{eq:Tn}, then $T(n) = O(n^4)$.
\end{proposition}

The above proposition says that if in the first invokation of Line \ref{line:DT} in Algorithm \ref{alg:psp_i_simplified}, the partition size $\abs{\P'}$ is no more than $\frac{n}{2}$ and the number of nodes of each subgraph is no more than $\frac{n}{2}$. Then our PSPI algorithm has $O(n^4)$ time complexity. This is general scenario and one order of magnitude faster than Algorithm \ref{alg:psp} $O(n^5)$.

If the worst case scenario is considered, that is, at most $\abs{V}-2$ times \texttt{Split} is called. Suppose the time complexity of \texttt{DT} is bounded by $Cn^4$. Then in worst scenario, the complexity bound for \texttt{Split} is $C(3^4+4^4 + \dots + n^4) \sim \frac{1}{5}Cn^5$, which is $5$ times faster than the original PSP algorithm $Cn^5$.
\subsection{Empirical Comparison of Different PSP Algorithms}
In this section, we compare our PSPI \ref{alg:psp_i_simplified} with Narayanan's algorithm \ref{alg:psp}.
Another improvement is formulated by \cite{kolmogorov} using parametric maximum flow.
This improvement can achieve $\abs{V} \textrm{MF}(G)$ time complexity theoretically. And we will compare the above three using two datasets with different density.

The first dataset is four Gaussian blobs with equal size and we can vary the number of points in each blob. The dataset with 100 points in total is the same with that used in 
Section \ref{subsec:dc}. The second dataset is a two-level graph with $s^3$ nodes. The dataset with $s=4$ is the same with that used in Section \ref{subsec:cd}.
We can construct a graph from the first dataset and the edge weight is computed with the rbf kernel. The characteristics of the two datasets are summarized in the following table:
\begin{table}[!ht]
\centering
\begin{tabular}{ccc}
\hline
name & weight type & density \\
\hline
GaussianBlobs & int & $|E|=O(|V|^2)$\\
Two-level graph & float & $|E|=O(|V|^{3/2})$\\
\hline
\end{tabular}
\end{table}
We use CPU times to measure the time complexity of the algorithms, by scaling the two kinds of graphs, the experiment results are shown in Figure \ref{fig:esc}.
As can be seen, our implementation is much faster than previous ones.

\begin{figure}
	\centering
	\begin{subfigure}{0.45\textwidth}
		\includegraphics[width=\textwidth]{pic/2019-08-26-gaussian.eps}
	\end{subfigure}
	\begin{subfigure}{0.45\textwidth}
		\includegraphics[width=\textwidth]{pic/2019-09-19-two_level.eps}
	\end{subfigure}
	\caption{Empirical speed comparison of different PSP algorithms}\label{fig:esc}
\end{figure}

We notice that parametric flow based PSP  \citep{kolmogorov}  does not perform well in practice. This may be related with the extra cost to construct intermediate graph structures and maintain concurrent threads, which can not be neglectable in implementation.

\section{Empirical Study of GBIC}\label{sec:es}
In this section, we apply GBIC to various machine learning problems including community detection, link prediction, data clustering and outlier detection. In some problems we need to give decision criterion  based on the structure of GBIC. These decision criterions are empirically inspired and performs well in our experiment.  Our experiments
include synthetic and real-world dataset and we will compare GBIC with classical methods and Bayesian inference.
\subsection{Data Clustering Scenario}\label{subsec:dc}
For data clustering task (or clustering analysis), we are given $n$ datas ,each with a feature vector of length $m$. To use GBIC on such scenario, we need to construct a similarity matrix from the feature vectors. Each element from the similarity matrix corresponds a weight value of the underlining graph. The Spectural Clustering follows the similarity matrix approach. Since distance metric is well studied in clustering analysis. It is often to use $\exp(-\gamma d)$ to transform the distance to similarity. The rbf kernel and Laplacian kernel are of this type. For these type of similarity, the graph is quite dense and to reduce the computational cost we can treat very small weights as zero. That is, we set a threshold for the weights to remove some edges in the graph. We can also use k-nearest neighbors to generate discrete similarities. It is hard to say which similarity metric is better than others and it depends on specific problems. Besides, the hyperparameters of the metric can be tuned as well.

After we get the clustering tree, we can get a proper flat clustering by threshold method. This is typical for most hierarhical clustering method. Then we can compare different clustering method, whether they are hierarical or not. We use 4 different datasets for such purpose.
\begin{enumerate}
\item Four Gaussian blobs with unit variance and centers at $(3,3), (3,-3), (-3,3),  \linebreak[4](-3,-3)$ respectively.  We generate 25 points for each blob.
\item Three concentric circles with radius $0.1,0.2,0.3$ and number of data points as $60, 100, 140$. The angles of these points are uniform distributed and they also oscillate along the radius direction.
\item UCI glass dataset with 214 samples, 6 classes, 9 dimensions of feature.
\end{enumerate}
We use adjusted rand index as the metric to score different clustering method, the best performance for each algorithm is shown in Table \ref{tb:e1}.  It can be seen from the table that graph-based info-clustering is better than other clustering algorithms.
\begin{table}[!ht]
\centering
\InputIfFileExists{compare_3.tex}{}{}
\caption{ accuracy for different clustering algorithms }\label{tb:e1}
\end{table}
To compare different hierarhical clustering method, we can compare the clustering tree directly. We use the gene-expression dataset coming from \cite{khan2001classification}.
Since we do not know the ground truth tree structure. We can split the fearues and apply the clustering algorithms on train and test data respectively and compare the resulting
clustering tree. In this way, the clustering tree from the train data is treated as ground truth. By doing so we can also test the stability of the clustering method.

We compare GBIC with agglomerative hierarhical clustering with average distance and Bayesian Rose tree \citep{blundell2011discovering}. The tree similarity metric is Robinson-Foulds distance, which is the number of steps to transform one tree to another (A step is either adding or removing an edge) \citep{day1985optimal}. We vary the samples of data and the results are shown in Figure \ref{fig:shc}. As can be seen, for the classical hierarhical method the trees are complex and the distance is large. The distance of GBIC and Bayesian Rose tree is near and our method is more suitable when the number of samples are relatively small.

\begin{figure}
\centering
\includegraphics[width=6cm]{pic/plot_results.eps}
\caption{Comparison of the stability of hierarhical clustering method}\label{fig:shc}
\end{figure}
\subsection{Community Detection}\label{subsec:cd}
Community Detection problems can be regarded as finding a proper partition of a given graph and we call each subgraph in the partition a community. There are many good algorithms
to find the flat partition which splits the graph into several non-overlapping subgraphs \citep{malliaros2013clustering}. However, it is a hard problem to make inference on the hierarhical structure of a complex graph automatically. In this subsection GBIC can achieve success to recover the hierarhical structure of a 2-layer graph upon properly defined ``edge weight''. 

We can not treat each edge with equal weight otherwise it is easy to obtain only trivial clustering results due to Proposition \ref{prop:triangle}. Inspired by Proposition \ref{prop:reweight}, we use the following weighting scheme:

\begin{equation}\label{eq:wij_scheme}
    w_{ij} = 1 + \abs{\{k | (i,k),(j,k) \in E \}} \textrm{ for } (i,j) \in E
\end{equation}
This weight assignment scheme is the number of triangles which contain the edge, which is equivalent to $r=n$ in the model problem of Proposition \ref{prop:triangle}. Then there are
at least $\frac{n^2}{2}$ edges to cut off the recovery condition, which is quite strong. We consider a more complex model and empirically justify the weighting scheme of Equation 
\eqref{eq:wij_scheme} works.

\begin{figure}
	\centering
	\begin{subfigure}{0.45\textwidth}
		\includegraphics[width=\textwidth]{pic/two_level.eps}
		\caption{a community with two hierarchical levels with $z_{\mathrm{in}_1} = 14,$ $z_{\mathrm{in}_2} = 3, z_{\mathrm{out}}=1$.}\label{fig:c1}
	\end{subfigure}
	\begin{subfigure}{0.45\textwidth}
		\includegraphics[width=\textwidth]{pic/tree_info-clustering.pdf}
		\caption{hierarchical tree obtained by graph-based info-clustering for left figure. The tree leaves with the same ancestor are merged for clarity.}\label{fig:c2}
	\end{subfigure}
	\caption{Community Detection Experiment Illustration}
\end{figure}
We use a two-level artificial dataset proposed in \cite{RN22}. 
The community structure is shown in Fig. \ref{fig:c1}. It has 4 communities in macro-level, and each macro community contains 4 micro communities. We use $z_{\mathrm{in}_1}$ to represent the internal average degree of nodes within micro community; $z_{\mathrm{in}_2}$ as node degree between different micro communities but within one macro community; $z_{\mathrm{out}}$ as node degree between different macro communities. By varying one and fixing the other two in $\{z_{\mathrm{in}_1}, z_{\mathrm{in}_2}, z_{\mathrm{out}} \}$ we get different set of experiment. To compare the deduced hierarchical tree structure with the ground truth, we use the normalized Robinson-Foulds distance as the metric, which lies within $[0,1]$. We compare the RF distance of GBIC with that of GN (Girvan-Newman algorithm) and BHCD (a kind of Bayesian Hierarchical method for graph structure in \cite{RN23}). The result is shown in Fig. \ref{fig:cdr}. As can be seen from Fig.\ref{fig:cdr}, in either case, graph-based info-clustering can produce more similar tree structures than that of the ground truth.
\begin{figure}
	\centering
	\begin{subfigure}{0.33\textwidth}
		\includegraphics[width=\textwidth]{pic/z_in_1.eps}
		\caption{}
	\end{subfigure}~
	\begin{subfigure}{0.33\textwidth}
		\includegraphics[width=\textwidth]{pic/z_in_2.eps}
		\caption{}
	\end{subfigure}~
	\begin{subfigure}{0.33\textwidth}
		\includegraphics[width=\textwidth]{pic/z_o.eps}
		\caption{}
	\end{subfigure}
	\caption{Distance comparison between different hierarchical community detection method. Smaller distance indicates more similar structure with the ground truth tree.}\label{fig:cdr}	
\end{figure}
\subsection{Link Prediction}
Link Prediction Problem is predicting whether an unobserved edge exists between two nodes in a graph \citep{liben2007link}.  GBIC does not give an explicit way to make prediction. Here we give a heuristic prediction scheme based on the hierarchical tree $T$. Our problem is that given node $(i,j) \not\in E$, we should make a prediction whether they have an connected edge. We split the scenario 
into two cases:
\begin{enumerate}
\item If they are share the same parent in the tree $T$, we test whether the addition of the edge with weight $w$ will split them. To be more specific, we consider the sub-graph consisting of all the sibling of $i,j$. We do GBIC on this sub-graph. If the partition is trivial, then we conclude that the connected edge exists. Otherwise, the edge does not exist.

\item If they do not share the same parent in the tree $T$, we test whether the addition of the edge will join them. The test is bidirectional. Suppose $I$ is the parent node of $i$ and it consists of several parts. Consider the sub-graph $I \cup \{j\}$. If the largest critical value becomes larger than before, then there is an edge bewteen $i,j$. Otherwise, consider the other part of the test for $J$ including $j$. If neither of the two tests succeed, we conclude that there is no edge between $i,j$. To test whether the largest critical value becomes larger than before, we will use Proposition \ref{prop:main},  thus avoiding complicated solution of GBIC for subgraphs.
\end{enumerate}

Co-authorship datasets are a typical examplar to compare different prediction methods. We use the NIPS-authorship (00-17) dataset as in \cite{RN23}.
Since the graph is sparse and it is hard to predict for some node with little connection information, we restrict the network to the 234 nodes whose degrees are larger than other cutting-off nodes. Since the prediction  is a hypothesis testing problem, we use true positive rate (TPR), true negative rate(TNR) and accuracy (ACC) as the evaluation metric. We compare GBIC with BHCD and a local prediction method based on a metric called Resource Allocation Index \citep{zhou2009predicting}. For TPR our method achieves higher score.
\begin{table}
\centering
\input{compare.tex}
\end{table}



\subsection{Outlier Detection}\label{subsec:od}
Outlier Detection is the identification of abnormal data (called outliers) which differs from the majority of the data (called inliers) \citep{grubbs1969procedures}.
To apply our GBIC method to this scanerio, we consider the case when we are required to detect outliers in a multi-class dataset and we are required to
extract a partition $\{\P_1, \dots, \P_K, \{x_1\}, \dots, \{x_M\}\}$from the PSP where $K$ is the number of clusters and $M$ is the number of outliers.
In reality it is often the case that we do not know the value of $K, M$.
If both $K$ and $M$ are small, an empirical and reasonable way to deal with this situation is inspecting the partition $\P_k, \P_{k-1}, \dots$.
The selection rule can be summarized as follows:

\begin{algorithmic}
\STATE Let $r=1$
\WHILE{ $ n \leq (r+1) |\mathcal{P}_{k-r}|$}
\STATE $r\leftarrow r+1$
\ENDWHILE
\end{algorithmic}

We then use $\P_{k-r}$ to distinguish different clusters and outliers. To predict new observations,  we use a decision rule from Proposition \ref{prop:main}.
Suppose $\gamma_{k-r}$ is the corresponding critical value of $\P_{k-r}, \P_{k-r+1}$ and rbf kernel is used as the similarity metric, then the equation of
the boundary curve can be written as
\begin{equation}
\sum_{j \in C_i} \exp(-\gamma \norm{x - x_j}^2)= \gamma_{k-r}, \text{ where } C_i \in \P_{k-r},  \abs{C_i} > 1
\end{equation}
Notice $C_i$ is one of clusters, and there are $K$ boundary curves in total. If the new observations falls outside all of these closed balls, then we classify it as an outlier.

We prepare three artifical datasets to verify our empirical method. The first one is one Gaussian blob with some noisy points uniformly distributed in a square. The second dataset is that we put $(0,0)$ to the Four Gaussian Blob dataset used in Section \ref{sec:es}. The last one is two misplaced semi-circles (``Two moons'') and some noisy points inside and outside. The weight is chosen by rbf kernel and the results are shown in Figure \ref{fig:boundary}. The boundary curve for Info-Detection is approximately the closure of the set of inliers.
\begin{figure}[!ht]
	\centering
	\includegraphics[width=\textwidth]{pic/outlier_boundary_illustration.eps}
	\caption{Detection boundary lines of different methods on different artificial dataset}	\label{fig:boundary}
\end{figure}

For outlier detection tasks, we also compare GBIC with other commonly used techniques on three datasets: GaussianBlob, Moon and UCI Lymphography. The other methods include local outlier factor \citep{Breunig}, isolation forest \citep{if}, elliptic envelope \citep{rousseeuw1999fast} and one class SVM \citep{svm}. We use two metrics to measure the overall performance of the detection: true positive rate (TPR) and true negative rate (TNR). The inlier is treated as positive sample. TPR measures the percentage of right detection in inlier set while TNR measures that in outlier set. It is difficult for a method to achieve high score on both two metrics. We manage to maximize TNR while controlling TPR $\geq 90\%$ for each method. The hyper parameters and metric are tuned for each method and the best result of TNR is shown in Table \ref{tab:odm}. We can see that Info-Detection is competitive with local outlier factor and outperforms other methods on these datasets.

\begin{table}
\centering
\input{id_compare.tex}
\caption{Comparison of different outlier detection methods}\label{tab:odm}
\end{table}

The method presented in this section is an extension of Info-Detection \citep{zhao2019info}, in which the inliers are assumed to be only one class. Therefore it can handle more general cases and
jointly do outliers and clustering tasks.

\section{Conclusion}\label{sec:conc}
In this paper, we consider a special model of info-clustering applied on graph strcture and propose a method called graph-based info-clustering (GBIC). We also propose an efficient algorithm to compute the principal sequence of partition of a graph, which is used to deduce the hierarhical structures of GBIC. Empirical study shows that GBIC can be applied to many fields and it performs well on a wide range of datasets. Our method is especially suitable if the hierarhical tree structure of data is prefered and to remove outliers from multiple-class datasets.
%\begin{acknowledgements}
%If you'd like to thank anyone, place your comments here
%and remove the percent signs.
%\end{acknowledgements}


% Authors must disclose all relationships or interests that 
% could have direct or potential influence or impart bias on 
% the work: 
%
% \section*{Conflict of interest}
%
% The authors declare that they have no conflict of interest.


% BibTeX users please use one of
\bibliographystyle{spbasic}      % basic style, author-year citations
%\bibliographystyle{spmpsci}      % mathematics and physical sciences
%\bibliographystyle{spphys}       % APS-like style for physics
\bibliography{exportlist.bib}   % name your BibTeX data base

% Non-BibTeX users please use
%\begin{thebibliography}{}
%
% and use \bibitem to create references. Consult the Instructions
% for authors for reference list style.
%
%\bibitem{RefJ}
% Format for Journal Reference
%Author, Article title, Journal, Volume, page numbers (year)
% Format for books
%\bibitem{RefB}
%Author, Book title, page numbers. Publisher, place (year)
% etc
%\end{thebibliography}
\appendix
\section{Proofs}
\subsection{Proof of Theorem \ref{thm:DPX}}
We need the following Lemma to prove Theorem \ref{thm:DPX}:
\begin{lemma}\label{lem:xyz}
	If $(X,Y)$ is weakly dependent with $Z$, then $X$ is weakly dependent with $Z$.
\end{lemma}
\begin{proof}
	From the definition of weakly dependence for two random variables, we have
	\begin{equation}
	P_{X,Y|Z=z}(x,y) = P_{X,Y}(x,y)(1+\epsilon \frac{\phi_z(x,y)}{\sqrt{P_{X,Y}(x,y)}}), z \in \mathcal{Z}
	\end{equation}
	Summing the above equation over $y\in \mathcal{Y}$ we have
	\begin{equation}
	P_{X|Z=z}(x) = P_X(x)(1+\epsilon\frac{\tilde{\phi}_z(x)}{\sqrt{P_X(x)}}),
	\textrm{ where } \tilde{\phi}_z(x) = \frac{\sum_{y\in Y} \sqrt{P_{X,Y}(x,y) }\phi_z(x,y)}{\sqrt{P_X(x)}}
	\end{equation}
\end{proof}
\begin{proof}[Proof of Theorem \ref{thm:DPX}]
From the definition of weak dependence for more than 2 random variables, we can find a discrete distribution $U$ over $\{1, 2,\dots, K\}$, such that $X_1, \dots, X_n$ are independent conditioned on $U$. Since $(X_1, \dots, X_n)$ is weakly dependent with $U$, from Lemma \ref{lem:xyz}, it follows that $X_i$ is weakly dependent with $U$.  That is, 
\begin{equation}\label{XUk}
P_{X_i | U=k}(x) = P_{X_i} (x)( 1 + \epsilon {\phi^{(k,i)}(x ) \over \sqrt{P_{X_i}(x)}} )
\end{equation}
We then have
\begin{align}
P_{X_i, X_j | U = k}(x_i, x_j)
=& P_{X_i | U=k}(x_i)
P_{X_j | U=k}(x_j) \notag \\
=& P_{X_i}(x_i)P_{X_j}(x_j)
(1 + \epsilon(\frac{\phi^{(k,i)}(x_i)}{P_{X_i}(x_i)}
+ \frac{\phi^{(k,j)}(x_j)}{P_{X_j}(x_j)}) +
\epsilon^2\frac{\phi^{(k,i)}(x_i)
\phi^{(k,j)}(x_j)}{P_{X_i}(x_i)P_{X_j}(x_j)})\label{eq:XiXj}
\end{align}
Since 
\begin{align*}
P_{X_i}(x) &= \sum_{k=1}^{K} P_{X_i | U=k}(x) P_U(u_k) \\
& =  \sum_{k=1}^{K}P_U(u_k)P_{X_i} (x)( 1 + \epsilon {\phi^{(k,i)}(x ) \over \sqrt{P_{X_i}(x)}} ) \textrm{ from } \eqref{XUk}\\
\Rightarrow & \sum_{k=1}^{K} P_U(u_k){\phi^{(k, i)}(x) \over \sqrt{P_{X_i}(x)}} =0,\forall i, x\in \mathcal{X}
\end{align*}
From \eqref{eq:XiXj} we have
\begin{equation}\label{eq:PXiXj}
P_{X_i, X_j}(x_i, x_j) = P_{X_i}(x_i)
P_{X_j}(x_j) (1+\epsilon^2 \sum_{k=1}^K P_U(u_k)
\frac{\phi^{(k,i)}(x_i)
\phi^{(k,j)}(x_j)}{P_{X_i}(x_i)P_{X_j}(x_j)})
)
\end{equation}
For more than 2 random variables:
\begin{align*}
P_{X_1,\dots,X_n}(x_1,\dots,x_n)  &= \sum_{k=1}^{K} P_{X_1,\dots,X_n | U=k}(x_1,\dots,x_n) P_U(u_k) \\
&=  \sum_{k=1}^{K}P_U(u_k) \prod_{i=1}^n P_{X_i|U=k}(x_i)\\
&= \sum_{k=1}^{K} P_U(u_k)\prod_{i=1}^n \left(P_{X_i} (x_i)( 1 + \epsilon {\phi^{(k,i)}(x_i ) \over \sqrt{P_{X_i}(x_i)}} )\right)\\
&=  \sum_{k=1}^{K}P_U(u_k) (\prod_{i=1}^n  P_{X_i} (x_i))
\left( 1 + \epsilon\sum_{i=1}^n {\phi^{(k,i)}(x_i) \over \sqrt{P_{X_i}(x_i)}} + \epsilon^2\sum_{i\neq j}{\phi^{k,i}(x_i)\phi^{k,j}(x_j)\over \sqrt{P_{X_i}(x_i)P_{X_j}(x_j)} }\right)+o(\epsilon^2) \\
&= (\prod_{i=1}^n  P_{X_i} (x_i))
(1+\epsilon\sum_{i=1}^n \sum_{k=1}^{K} P_U(u_k){\phi^{(k,i)}(x_i) \over \sqrt{P_{X_i}(x_i)}} \\
&+\epsilon^2 \sum_{k=1}^{K} P_U(u_k)\sum_{i\neq j}{\phi^{k,i}(x_i)\phi^{k,j}(x_j)\over \sqrt{P_{X_i}(x_i)P_{X_j}(x_j)} } ) + o(\epsilon^2)\\
&= (\prod_{i=1}^n  P_{X_i} (x_i))
\left(1 +\epsilon^2\sum_{i\neq j} \sum_{k=1}^{K}P_U(u_k){\phi^{k,i}(x_i)\phi^{k,j}(x_j)\over \sqrt{P_{X_i}(x_i)P_{X_j}(x_j)} }\right) + o(\epsilon^2)
\end{align*}
From \eqref{eq:PXiXj},
let $\widetilde{B}_{ij}(x_i, x_j)={P_{X_i, X_j}(x_i,x_j) - P_{X_i}(x_i)P_{X_j}(x_j) \over \sqrt{P_{X_i}(x_i)P_{X_j}(x_j)}} $, then we have:
\begin{align}
\epsilon^2\sum_{k=1}^{K}P_U(u_k)
{\phi^{k,i}(x_i)\phi^{k,j}(x_j)\over \sqrt{P_{X_i}(x_i)P_{X_j}(x_j)} } & = {P_{X_i, X_j}(x_i, x_j) - P_{X_i}(x_i)P_{X_j}(x_j) \over P_{X_i}(x_i)P_{X_j}(x_j)} \notag\\
& = {\widetilde{B}_{ij}(x_i, x_j) \over \sqrt{P_{X_i}(x_i)P_{X_j}(x_j)} } \label{eq:Bsecond}
\end{align}
Therefore, we have 
\begin{equation}\label{eq:sep}
P_{X_1,\dots,X_n}(x_1,\dots,x_n) =  (\prod_{i=1}^n  P_{X_i} (x_i))\left ( 1 + \sum_{i\neq j}{\widetilde{B}_{ij}(x_i, x_j) \over \sqrt{P_{X_i}(x_i)P_{X_j}(x_j)} }\right) +o(\epsilon^2)
\end{equation}
Then $P_{X_1,\dots, X_n}$ is in the neighborhood of $P_{X_1}\dots P_{X_n}$ with $$\phi(x_1,\dots, x_n)=
\sqrt{P_{X_1}(x_1)\dots P_{X_n}(x_n)}\left(\sum_{i\neq j}{\widetilde{B}_{ij}(x_i, x_j) \over \sqrt{P_{X_i}(x_i)P_{X_j}(x_j)} }\right)+o(\epsilon^2)$$

Using Equation \eqref{eq:approx:ig} from information geometry:
\begin{align*}
D(P_{X_1,\dots, X_n}|| P_{X_1}\dots P_{X_n}) & ={1 \over 2} \sum_{x_1,\dots,x_n}\phi^2(x_1,\dots, x_n) \\
& = {1\over 2}\sum_{x_1,\dots,x_n} (\prod_{i=1}^n  P_{X_i} (x_i)) \left(\sum_{i\neq j}{\widetilde{B}_{ij}(x_i, x_j) \over \sqrt{P_{X_i}(x_i)P_{X_j}(x_j)} }\right)^2 +o(\epsilon^2) 
\end{align*}
From the definition of $\widetilde{B}_{ij}$, the above is the summation over $\norm{\widetilde{B}_{ij}}^2_F$(the coupling term is zero). Therefore we get the second order approximation for a total partition:
\begin{equation}
D(P_{X_1,\dots, X_n}|| P_{X_1}\dots P_{X_n}) =   {1 \over 2} \sum_{i\neq j} \norm{\widetilde{B}_{ij}}^2_F + o(\epsilon^2)
\end{equation}
For an arbitrary partition $\P$, using Equation \eqref{eq:sep} for each $C\in \P$, we have
\begin{equation}
P_{X_C}(x_C) = \prod_{i\in C} P_{X_i}(x_i)(1+\sum_{i\neq j,i,j\in C} \frac{\tilde{B}_{ij}(x_i, x_j)}{\sqrt{P_{X_i}(x_i)P_{X_j}(x_j)}}) + o(\epsilon^2)
\end{equation}
Multiplying these equations and notice $\frac{\tilde{B}_{ij}(x_i, x_j)}{\sqrt{P_{X_i}(x_i)P_{X_j}(x_j)}}=O(\epsilon^2)$ from
Equation \eqref{eq:Bsecond} we have:
\begin{equation}
\prod_{C\in \P}P_{X_C}(x_C) = \prod_{i=1}^n P_{X_i}(x_i)(1+\sum_{C\in\P}\sum_{i\neq j,i,j\in C}\frac{\tilde{B}_{ij}(x_i, x_j)}{\sqrt{P_{X_i}(x_i)P_{X_j}(x_j)}}) + o(\epsilon^2)
\end{equation}
Then $\prod_{C\in \P}P_{X_C}$ is in the neighborhood of $P_{X_1}\dots P_{X_n}$ with $$\phi_{\P}(x_1,\dots, x_n)=
\sqrt{P_{X_1}(x_1)\dots P_{X_n}(x_n)}\left(\sum_{C\in\P}\sum_{i\neq j,i,j\in C}\frac{\tilde{B}_{ij}(x_i, x_j)}{\sqrt{P_{X_i}(x_i)P_{X_j}(x_j)}}\right)+o(\epsilon^2)$$.
Applying Equation  \eqref{eq:approx:ig} we have:
\begin{align*}
D(P_{X_1,\dots, X_n}|| \prod_{C\in \P}P_{X_C}) & ={1 \over 2} \sum_{x_1,\dots,x_n}(\phi(x_1,\dots, x_n)-\phi_{\P}(x_1, \dots, x_n))^2 \\
& = {1\over 2}\sum_{x_1,\dots,x_n} (\prod_{i=1}^n  P_{X_i} (x_i)) \left(\sum_{\substack{(i,j) \not\in C\\ C\in \P}} {\widetilde{B}_{ij}(x_i, x_j) \over \sqrt{P_{X_i}(x_i)P_{X_j}(x_j)} }\right)^2 +o(\epsilon^2) \\
& = \frac{1}{2} \sum_{\substack{(i,j) \not\in C\\ C\in \P}} \norm{\widetilde{B}_{ij}}_F^2 + o(\epsilon^2)
\end{align*}
\end{proof}
\subsection{Proof of Proposition \ref{prop:triangle}}
\begin{proof}[Proof of Proposition \ref{prop:triangle}]

We use deduction to show  
\begin{equation}\label{eq:GF}
f[\P] \geq \frac{\abs{\P}-1}{\abs{V}-1} \sum_{(i,j) \in E} w_{ij}
\end{equation}
for any $\P \in \Pi$. Then by Theorem \ref{thm:trival} in the paper, Proposition \ref{prop:triangle} holds. 

Let $n=\abs{V}$. For $\abs{\P}=n$, the equality of equation \ref{eq:GF} holds. 

Suppose equation \eqref{eq:GF} holds for any $\abs{\P} \geq k+1(k\geq 2)$, and for $\P=\{C_1, \dots, C_k\}$, with $\abs{C_1} \leq \abs{C_r}$ for $r=2,\dots, k$. Let $\abs{C_1}=n_1(\geq 2), \P_{C_1} = \{\{i\}| i \in C_1\}, \P'=\P_{C_1} \cup \P \backslash \{C_1\}$. Then we have $\abs{\P'} = k+n_1-1$. Using equation \eqref{eq:GF} for $\P'$ we have
\begin{align}\label{eq:PPrelation}
f[\P'] & = \sum_{i\in C_1, k \not\in C_1} w_{ik} + \sum_{r=2}^k \sum_{(i,j) \in E(C_r)} w_{ij}\geq \frac{k+n_1 -2}{n-1}\sum_{(i,j) \in E} w_{ij} \\
f[\P] & =f[P'] - \sum_{(i,j) \in E(C_1)} w_{ij}
\end{align}
Applying triangle inequality $w_{ij} \leq w_{ik} + w_{jk}$ for given $k\not\in C_1$ and sum it over all $i, j \in C_1, i\neq j$, we have
$$
\sum_{(i,j) \in E(C_1)} w_{ij} \leq \sum_{(i,j) \in E(C_1)} (w_{ik} + w_{jk}) = (n_1-1)\sum_{i\in C_1} w_{ik}
$$
Summation over $k \not\in C_1$ we have 
$$
(n - n_1) \sum_{(i,j) \in E(C_1)} w_{ij} \leq (n_1 - 1) \sum_{i \in C_1, k \not\in C_1} w_{ik}
$$
Also
\begin{align*}
\sum_{(i,j) \in E} w_{ij}  & \geq \sum_{(i,j) \in E(C_1)} w_{ij} + \sum_{i\in C_1, k\not\in C_1} w_{ik} \\
(n_1 - 1)\sum_{(i,j) \in E} w_{ij}  & \geq (n_1 -1 )\sum_{(i,j) \in E(C_1)} w_{ij} + (n_1-1)\sum_{i\in C_1, k\not\in C_1} w_{ik} \\
& \geq (n_1 -1 )\sum_{(i,j) \in E(C_1)} w_{ij} + (n - n_1) \sum_{(i,j) \in E(C_1)} w_{ij}\\
& = (n-1) \sum_{(i,j) \in E(C_1)} w_{ij}
\end{align*}
We then have $\sum_{(i,j) \in E(C_1)} w_{ij} \leq \frac{n_1-1}{n-1}\sum_{(i,j) \in E} w_{ij}$. From equation \eqref{eq:PPrelation} we have 
$f[\P] \geq \frac{k-1}{n-1}\sum_{(i,j) \in E} w_{ij}$. That is, the result holds for $\abs{\P}=k$.
\end{proof}
\subsection{Proof of Proposition \ref{prop:main}}
\begin{proof}[Proof of Proposition \ref{prop:main}]
		In this proof, we make some notation convention.
		$w(C) = \displaystyle\sum_{(i,j) \in E(C)} w_{ij},$
		$w(A, C) = \displaystyle\sum_{\substack{i \in A, j \in C \\ (i,j) \in E(V')}} (w_{ij}+w_{ji})$ and
		$\P_C = \{\{i \}| i \in C \}$. Suppose $\gamma_N = I(Z_B)$, from equation \eqref{eq:largest_threshold}, we can get $\gamma_N =\frac{f[\P_B]} {\abs{B} -1}$.
		Since $V' = B \cup \{i'\}$
		$f[\P_{V'}] = f[\P_B] + \sum_{i \in B}w_{ii'}$.
		
		If $ \sum_{i \in B} w_{ij} > \gamma_N$ we can get
		$$
		\gamma'_N \geq I_{\P_{V'}}(Z_{V'}) = \frac{(\abs{B}-1)\gamma_N + \sum_{i \in B}w_{ii'}}{\abs{B}} > \gamma_N
		$$
		
		On the other hand, suppose $\gamma'_N = I(Z_K) > \gamma_N$. Then $K$ must contain $i'$. If $K=V'$ then the conclusion holds. Otherwise, Suppose $K = \{i'\} \cup B', B=B'\cup J, J\neq \emptyset$. We can write 
		\begin{equation}\label{eq:gammaNF}
		\gamma_N = \frac{w(J,B') + w(J) + w(B')}{ \abs{B'} + \abs{J} - 1 }
		\end{equation}
		Since $I(Z_K) = I_{\P_K}(Z_K)$ is maximal, we have $I_{\P_K}(Z_K) > I_{\P_{V'}}(Z_{V'})$ and $I_{\P_K}(Z_K) \geq I_{\P_{B'}}(Z_{B'})$.
		\begin{align*}
			\frac{w(\{i'\}, B') + w(B')}{\abs{B'}} >& \frac{w(B') + w(J, B') + w(J) + w(\{i'\}, B') + w(\{i'\}, J)}{\abs{B'} + \abs{J}}  \\
			\frac{w(\{i'\}, B') + w(B')}{\abs{B'}} \geq & \frac{w(B')}{\abs{B'} - 1}
		\end{align*}
		we can get 
		\begin{align}
			\abs{J} (w(\{i'\}, B') + w(B')) > & \abs{B'} (w(J, B') + w(J) + w(\{i'\}, J)) \label{eq:target}
			\\
			(\abs{B'} - 1)  w(\{i'\}, B') \geq & w(B') \label{eq:convert}
		\end{align}
		Take equation \eqref{eq:convert} in \eqref{eq:target}, we can get
		\begin{equation}\label{eq:summation}
		\abs{J} w(\{i'\}, B') > w(J, B') + w(J) + w(\{i'\}, J)
		\end{equation}	
		Combined with \eqref{eq:gammaNF}, adding equation \eqref{eq:convert} and \eqref{eq:summation} we can get
		$w(\{i'\}, B') > \gamma_N$. Then $\sum_{j \in B}w_{ii'} > \gamma_N $ follows.
\end{proof}

\subsection{Proof of Proposition \ref{prop:reweight}}
We need the following lemmas to prove Proposition \ref{prop:reweight}.
\begin{lemma}[Corollary 5.1 in \cite{secretKey}]\label{lemma:laminarity}
	\begin{equation}\label{eq:P}
	I(Z_{C_1 \cup C_2}) \geq \min\{ I(Z_{C_1}), I(Z_{C_2})\}, \textrm{ for } C_1\cap C_2 \neq \emptyset
	\end{equation}
\end{lemma}
\begin{lemma}\label{lemma:sub}
	If node set $S_1 \cap S_2 \neq \emptyset, S_1 \not\subseteq S_2$ and $I(Z_{S_1}) \geq I(Z_{S_2})$, then $S_2$ is not a cluster of $G$.
\end{lemma}
\begin{proof}
	From Lemma \ref{lemma:laminarity},
	$I(Z_{S_1\cup S_2}) \geq \min\{I(Z_{S_1}), I(Z_{S_2})\} = I(Z_{S_2})$. From bottom-up formulation,  $S_2$ cannot be a cluster of $G$.
\end{proof}
\begin{corollary}\label{cor:complete}
	For a complete graph $G(V,E)$ with equal weight $w$ on each edge, we have $I(Z_{V})=\frac{nw}{2}$ where $n=\abs{V}$. And $I(Z_S) < I(Z_V)$ for any subset $S\subsetneq V$.
\end{corollary}
\begin{proof}
	From Proposition \ref{prop:triangle}, $I(Z_{V})$ is achieved by $\P=\{\{1\},\dots,\{n\} \}$ and $I(Z_V) = I_{\P}(Z_V) = \frac{n(n-1)w/2}{n-1} = \frac{nw}{2} $. From bottom-up approach, $I(Z_V) = \max_{S\subset V} I(Z_S)$, therefore $I(Z_S) \leq I(Z_V)$. If $I(Z_S) = I(Z_V)$, consider $\P_S=\{i | i \in S\}$, then $I(Z_S) = I_{\P_S}(Z_S) = \frac{\abs{S}w}{2} < I(Z_V)$, a contradiction. Therefore, $I(Z_S) < I(Z_V)$ holds for any subset $S\subsetneq V$.
\end{proof}
\begin{proof}[Proof of Proposition \ref{prop:reweight}]
	We will show that if $U \neq S_1$ and $U \neq S_2$, $U$ cannot be a cluster of $G$.
	
For any $U \in S_1 \cup S_2 $ and $\abs{U} \leq n$, If $U \in S_1$ or $U \in S_2$, then by Corollary \ref{cor:complete}, $I(Z_U) < I(Z_{S_1})$.
If $U \cap S_1 \neq \emptyset$ or $U \cap S_2 \neq \emptyset$ holds, by Lemma \ref{lemma:sub}, $I(Z_{S_1})= I(Z_{S_2}) > I(Z_U) \Rightarrow U$ is not a cluster of $G$.

For $\abs{U} > n$ but $U \neq V = S_1 \cup S_2$. Suppose $S_2 \subset U$. Let $\P' = \{\{i\}| i\in U\}$, $I(Z_U) \leq I_{\P'}(Z_U) = \frac{\frac{rn(n-1)}{2} + m + \frac{rk(k-1)}{2}}{n+k-1}$ where $m_1 \leq nk$ and $k=\abs{U} - n < n$. 
We compare it to $I(Z_{S_1}) = \frac{nr}{2}$ and we can show that $I(Z_{S_1}) - I_{\P'}(Z_U) \geq \frac{nrk-2m-rk(k-1)}{2(n+k-1)}$. 

If $m < \frac{nr}{2}$, then $I(Z_{S_1}) - I_{\P'}(Z_U) > \frac{r(n-k)(k-1)}{2(n+k-1)} \geq 0$. Then $I(Z_{S_1})>I(Z_U)$, $U$ is not a cluster by Lemma \ref{lemma:sub}. In either case, $U$ is not a cluster of $G$ for $1<\abs{U}<2n, U \neq S_1, U\neq S_2$. 

If $m \geq \frac{nr}{2}$, we have $I_{\P}(Z_V) = \frac{m+rn(n-1)}{2n-1} \leq m = I_{\{S_1, S_2\}}(Z_V)$. Suppose $I_{\P^*}(Z_V)$, If $S\in \P^*$ and $S\subseteq S_1$. From Lemma \ref{lemma:sub} we must have $S=S_1$. Then $\P^*=\{S_1, S_2\}$, which contradicts with
$I_{\P}(Z_V)  \leq  I_{\{S_1, S_2\}}(Z_V)$ since $\P$ is finer than $\{S_1, S_2\}$. Therefore, either $S_1$ or $S_2$ cannot be discoveried.

\end{proof}
\subsection{Proof of Proposition \ref{prop:alg_complexity}}
\begin{proof}[Proof of Proposition \ref{prop:alg_complexity}]
Let $\mu_i = \frac{n_i}{n} \leq \frac{1}{2}$. We proceed by induction to show $T(n) = O(n^4)$. More specifically, $T(n) \leq q C n^4$ where $ q = \frac{16}{5}$. $T(3)$ is a constant and $T(m) \leq q C m^4$ holds for $m=3$. Suppose
	$T(m) \leq qC m^4$ holds for all $m \leq n-1$. Then for $T(n)$
	We first show that 
	\begin{equation}\label{eq:outerI}
	\sum_{i=1}^k T(n_i) \leq 10 T(\frac{n}{2})
	\end{equation}
	Since $\sum_{i=1}^k T(n_i) \leq qC n^4\sum_{i=1}^k u_i^4$ and $10 T(\frac{n}{2}) \geq 10Cn^4 (\frac{1}{2})^4$ from equation \eqref{eq:Tn}
	\begin{equation}\label{eq:innerI}
       q\sum_{i=1}^k u_i^4 \leq 10 (\frac{1}{2})^4 
	\end{equation}
	We have \eqref{eq:innerI} $\Rightarrow$ \eqref{eq:outerI}. The constraint is that $u_1\leq u_2 \leq \dots \leq u_k \leq \frac{1}{2}$ and $\sum_{i=1}^k u_i = 1$. Therefore we have $u_1 \leq \frac{1}{k}, u_2 \leq \frac{1}{k-1}, \dots, u_{k-1} \leq \frac{1}{2}$.
	\begin{equation}\label{eq:outerOne}
	 q[2(\frac{1}{2})^4 + \sum_{i=3}^k (\frac{1}{i})^4] \leq 10 (\frac{1}{2})^4
	\end{equation}
	We have \eqref{eq:outerOne} $\Rightarrow$ \eqref{eq:innerI}. And \eqref{eq:outerOne} is equivalent to
	$\sum_{i=3}^k (\frac{1}{i})^4 \leq \frac{9}{8}(\frac{1}{2})^4$
	\begin{align*}
		\sum_{i=3}^k (\frac{1}{i})^4 & < \frac{1}{9}\sum_{i=3}^k (\frac{1}{i})^2 \\
		& < \frac{1}{9}\sum_{i=3}^k (\frac{1}{(i-1)i}) \\
		& < \frac{1}{18} < \frac{9}{8}(\frac{1}{2})^4
	\end{align*}
	Therefore, \eqref{eq:outerI} holds and from \eqref{eq:Tn} and $T(k) \leq T(\frac{n}{2})$ we have 
	\begin{align}
		T(n)  & \leq Cn^4 + 11T(\frac{n}{2}) \\
		& \leq C n^4 + 11 q C (\frac{n}{2})^4 = \frac{16}{5} C n^4
	\end{align}
\end{proof}	
\end{document}
% end of file template.tex

